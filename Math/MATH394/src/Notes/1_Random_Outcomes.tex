\section{Experiments with Random Outcomes}

    \subsection{Sample Spaces and Probabilities}            % 1.1

        \textbf{Sample space $\Omega$:} the set of all possible outcomes of the experiment. \\
        $\bullet$ elements of $\Omega$ are called \textbf{sample points} and typically denoted by $\omega$ \\
        $\bullet$ subsets of $\Omega$ are called \textbf{events}. The collection of events in $\Omega$ is denoted by $\mathcal{F}$. \\a
        $\bullet$ the \textbf{probability measure} or \textbf{probability distribution} or \textbf{probability} $P$ is a function from $\mathcal{F}$ into the real numbers. Each event $A$ has a probability $P(A)$, and
        $P$ satisfies the following axioms: \\
        (a) $0 \leq P(A) \leq 1$ for each event A \\
        (b) $P(\Omega) = 1$ and $P(\emptyset) = 0$. \\
        (c) If $A_1, A_2, A_3, \dots$ is a sequence of pairwise disjoint events then

        \[
            P\left(\cup_{i=1}^{\infty} A_i\right) = \sum^{\infty}_{i=1} P(A_i)
        \]

        The triple $(\Omega, \mathcal{F}, P)$ is called a \textbf{probability space}. Every mathematically precise model of a random experiment or collection of experiments must be of this kind.  \\

        The three axioms related to the probability measure $P$ above are known as \textbf{Kolmogorov's Axioms}. \textbf{Pairwise disjoint} means that $A_i \cap A_j = \emptyset$ for each pair of indices $i\not = j$, i.e.
        the events $A_i$ are \textbf{mutually exclusive}. Axiom (iii) says that the probability of the union of the mutually exclusive events is equal to the sum of their probabilities. Note that rule (iii) applies also
        to finitely many events. \\

        If $A_1, A_2, \dots, A_n$ are pairwise disjoint events then

        \[
            P(A_1 \cup \dots \cup A_n) = P(A_1) + \dots + P(A_n)
        \]

        \textit{\blue{Example:}} We flip a fair coin. The sample space is $\Omega = \{H, T\}$ (H for heads and T for tails). We take $\mathcal{F} = \{\emptyset, \{H\}, \{T\}, \{H, T\}\}$, the collection of all subsets of
        $\Omega$. The term "fair coin" means that the two outcomes are equally likely. So the probabilities of the singletons $\{H\}$ and $\{T\}$ are

        \[
            P\{H\} = P\{T\} = \frac{1}{2}
        \]

        By axiom (ii) we have $P(\emptyset) = 0$ and $P\{H, T\} = 1$. \\

        \textit{\blue{Example 2:}} We roll a standard six-sided die. Then the sample space is $\Omega = \{1, 2, 3, 4, 5, 6\}$. Each sample point $\omega$ is an integer between 1 and 6. If the die is fair then each outcome
        is equally likely, i.e.

        \[
            P\{1\} = P\{2\} = P\{3\} = P\{4\} = P\{5\} = P\{6\} = \frac{1}{6}
        \]

        A possible event in this sample space is

        \[
            A = \{\text{the outcome is even}\} = \{2, 4, 6\}
        \]

        Then

        \[
            P(A) = P\{2, 4, 6\} = P\{2\} + P\{4\} + P\{6\} = \frac{1}{2}
        \]

    \subsection{Random Sampling}                            % 1.2

        If the sample space $\Omega$ has finitely many elements and each outcome is equally likely then for any event $A \subset \Omega$ we have

        \[
            P(A) = \frac{#A}{#\Omega}.
        \]

        \textit{\blue{Example:}} Suppose our urn contains 5 balls labeled 1, 2, 3, 4, 5. Sample 3 balls with replacement and produce an ordered list of the numbers drawn. At each step we have the same 5 choices.
        The sample space is

        \[
            \Omega = \{1, 2, 3, 4, 5\}^3 = \{(s_1, s_2, s_3): \text{ each } s_i\in \{1, 2, 3, 4, 5\}\}
        \]

        and $#\Omega = 5^3$. Since all outcomes are equally likely, we have for example

        \[
            P\{\text{the sample is (2, 1, 5)}\} = P\{\text{the sample is (2, 2, 3)}\} = 5^{-3} = \frac{1}{125}
        \]

        \textit{\blue{Example 2:}} Consider again the urn with 5 balls labeled 1, 2, 3, 4, 5. Sample 3 balls without replacement and produce an ordered list of the numbers drawn. Now the sample space is

        \[
            \Omega = \{(2_1, s_2, s_3) : \text{ each } s_i \in \{1, 2, 3, 4, 5\} \tet{ and } s_1, s_2, s_3 \text{ are all distinct}
        \]

        The first ball can be chosen in 5 ways, the second ball in 4 ways, and the third ball in 3 ways. So

        \[
            P\{\text{the sample is (2,1,5)}\} = \frac{1}{5\cdot 4\cdot 3} = \frac{1}{60}
        \]

        The outcome (2, 2, 3) is not possible because repetition is not allowed. \\

        \textit{\blue{Example 3:}} Suppose our urn contains 5 balls labeled 1, 2, 3, 4, 5. Sample 3 balls without replacement and produce an unordered set of 3 numbers as the outcome. The sample space is

        \[
            \Omega = \{\omega : \omega \text{ is a 3-element subset of } \{1, 2, 3, 4, 5\}\}
        \]

        For example

        \[
            P(\text{the sample is }\{1, 2, 5\}) = \frac{1}{\binom{5}{3}} = \frac{2!\cdot 3!}{5!} = \frac{1}{10}
        \]

        The outcome $\{2, 2, 3\}$ does not make sense as a set of three numbers because of the repetition.

    \subsection{Infinitely Many Outcomes}                   % 1.3

        \textit{\blue{Example:}} Flip a fair coin until the first tails comes up. Record the number of flips required as the outcome of the experiment. What is the space $\Omega$ of possible outcomes? The number of flips
        needed can be any positive integer, hence $\Omega$ must contain all positive integers. We can also imagine the scenario where tails never comes up. This outcome is represented by $\infinity$. Thus

        \[
            \Omega = \{\infty, 1, 2, 3, \dots\}.
        \]

        The outcome is $k$ iff the first $k-1$ flips are heads and the $k$th flip is tails. This is one of the $2^k$ equally likely outcomes when we flip a coin $k$ times, so the probability of this event is $2^{-k}$.
        Thus

        \[
            P\{k\} = 2^{-k} \text{ for each positive integer $k$}
        \]

        This equation defines the \textbf{geometric probability distribution} with success parameter $\frac{1}{2}$ on the positive integers. The probability $P\{\infty\}$ can be derived from the axioms of probability:

        \[
            1 = P(\Omega) = P\{\infty, 1, 2, 3\dots\} = P\{\infty\} + \sum^{\infty}_{k=1} P\{k\}
        \]

        Because $P\{k\} = 2^{-k}$ for each positive integer $k$, we have

        \[
            \sum^{\infty}_{k=1} P\{k\} = \sum^{\infty}_{k=1} 2^{-k} = 1,
        \]

        which implies that $P\{\infty\} = 0$. \\

        \textit{\blue{Example 2:}} Consider a dartboard in the shape of a disk with a radius of 9 inches. The bullseye is a disk of diameter $\frac{1}{2}$ inch in the middle of the board. What is the probability that a
        dart randomly thrown on the board hits the bullseye? Let us assume that the dart hits the board at a uniformly chosen random location, that is, the dart is equally likely to hit anywhere on the board. \\

        The sample space is a disk of radius 9. For simplicity take the center as the origin of our coordinate system, so

        \[
            \Omega = \{(x,y): x^2 + y^2 \leq 9^2\}
        \]

        Let $A$ be the event that represents hitting the bullseye. This is the disk or radius $\frac{1}{4}:A = \left{(x,y): x^2 + y^2 \leq \left(\frac{1}{4}\right)^2\right}$. The probability should be uniform on the disk
        $\Omega$, thus

        \[
            P(A) = \frac{\text{area of }A}{\text{area of }\Omega} = \frac{\pi \left(\frac{1}{4}\right)^2}{\pi\cdot 9^2} = \frac{1}{36^2} \approx 0.00077
        \]

        The set $\Omega = \{\infty, 1, 2, 3, \dots\}$ is \textbf{countably infinite}, meaning that its elements can be arranged in a sequence, or equivalently, labeled by positive integers. A countably infinite sample
        space works like a finite sample space. To specify a probability measure $P$, it is enough to specify the probabilities of the outcomes and then derive the probability of each event by additivity:

        \[
            P(A) = \sum_{\omega: \omega \in A} P\{\omega\} \text{ for any event }A \subset \Omega
        \]

        Finite and countably infinite sample spaces are both called \textbf{discrete sample spaces}.

    \subsection{Consequences of the Rules of Probabaility}  % 1.4

        Additivity property: if $A_1, A_2, A_3, \dots$ are pairwise disjoint events and $A$ is their union, then $P(A) = P(A_1) + P(A_2) + P(A_3) + \dots$. Calculation of the probability of a complicated event $A$ almost
        always involving decomposing $A$ into smaller disjoint pieces whose probabilities are easier to find. \\

        \textit{\blue{Example:}} An urn contains 30 red, 20 green, and 10 yellow balls. Draw two without replacement. What is the probability that the sample contains exactly one red or exactly one yellow? \\

        We have

        \[
            P(\text{exactly one red or exactly one yellow}) = P(\text{red and green}) + P(\text{yellow and green}) + P(\text{red and yellow})
        \]

        Counting favorable arrangements for each of the simpler events gives:

        \begin{align*}
            P(\text{red and green}) &= \frac{30\cdot 20}{\binom{60}{2}} = \frac{20}{59} \\
            P(\text{yellow and green}) &= \frac{10\cdot 20}{\binom{60}{2}} = \frac{20}{177} \\
            P(\text{red and yellow}) &= \frac{30\cdot 10}{\binom{60}{2}} = \frac{10}{59}
        \end{align*}

        which leads to

        \[
            P(\text{exactly one red or exactly one yellow}) = \frac{20}{59} + \frac{20}{177} + \frac{10}{59} = \frac{110}{177}
        \]

        We used unordered samples but we can get the answer also by using ordered samples. \\

        \textit{\blue{Example 2:}} Peter and Mary take turns rolling a fair die. If Peter rolls 1 or 2 he wins and the game stops. If Mary rolls 3, 4, 5, or 6, she wins and the game stops. They keep rolling in turn
        until one of them wins. Suppose Peter rolls first. \\

        (a) What is the probability that Peter wins and rolls at most 4 times? \\

        For this event to occur, Peter must win on his first roll, win on his second roll, win on his third roll, or win on his fourth roll. These alternatives are mutually exclusive. So define events

        \[
            A = \{\text{Peter wins and rolls at most 4 times}\}
        \]

        and $A_k = \{\text{Peter wins on his $k$th roll}\}$. Then $A = \cup^4_{k=1} A_k$ and since the events $A_k$ are mutually exclusive, $P(A) = \sum^4_{k=1} P(A_k)$. To find the probabilities $P(A_k)$ we need to think
        about the game and the fact that Peter rolls first. Peter wins on his $k$th roll if first both Peter and Mary fail $k-1$ times and then Peter succeeds. Each roll has 6 possible outcomes. Peter's roll fails in 4
        different ways and Mary's roll fails in 2 different ways. Peter's $k$th roll succeeds in 2 different ways. Thus the ratio of the number of favorable alternatives over the total number of alternatives gives

        \[
            P(A_k) = \frac{(4\cdot 2)^{k-1} \cdot 2}{(6\cdot 6)^{k-1}\cdot 6} = \left(\frac{8}{36}\right)^{k-1} \frac{2}{6} = \left(\frac{2}{9}\right)^{k-1} \frac{1}{3}
        \]

        The probability asked is now obtained from a finite geometric sum:

        \begin{align*}
            P(A)    &= \sum^4_{k=1} P(A_k) \\
                    &= \sum^4_{k=1} \left(\frac{2}{9}\right)^{k-1} \frac{1}{3} \\
                    &= \frac{1}{3}\sum^3_{j=0} \left(\frac{2}{9}\right)^j \\
                    &= \frac{1}{3}\cdot \frac{1-\left(\frac{2}{9}\right)^4}{1-\frac{2}{9}} \\
                    &= \frac{3}{7}\left(1-\left(\frac{2}{9}\right)^4\right)
        \end{align*}

        Complements: Events $A$ and $A^C$ are disjoint and together make up $\Omega$, no matter what the event $A$ happens to be. Consequently

        \[
            P(A) + P(A^C) = 1
        \]

        A larger event must have larger probability:

        \[
            \text{if } A \subseteq B \text{ then } P(A) \leq P(B)
        \]

        \begin{axiom}{Inclusion-exclusion formulas for two and three events}
            \begin{align*}
                P(A \cup B) &= P(A) + P(B) - P(A\cap B) \\
                P(A \cup B \cup C)  &= P(A) + P(B) + P(C) - P(A\cap B) - P(A \cap C) - P(B \cap C) + P(A \cap B \cap C)
            \end{align*}
        \end{axiom}

        \textit{\blue{Example 1 (revisited):}} We solved this problem earlier by decomposing the event. Now apply first inclusion-exclusion and then count favorable arrangements using unordered samples:

        \begin{align*}
            P(\text{exactly one red or exactly one yellow}) &= P(\{\text{exactly one red}\}) \cup \{\text{exactly one yellow}\}) \\
                                                            &= P(\text{exactly one red}) + P(\text{exactly one yellow}) - P(\text{exactly one red and exactly one yellow}) \\
                                                            &= \frac{30\cdot 30}{\binom{60}{2}} + \frac{10\cdot 50}{\binom{60}{2}} - \frac{30\cdot 10}{\binom{60}{2}} \\
                                                            &= \frac{110}{177}
        \end{align*}

        \begin{axiom}{General inclusion-exclusion formula}
            P(A_1 \cup \dots \cup A_n)  &= \sum^n_{i=1} P(A_i) - \sum_{1 \leq i_1 \leq i_2 \leq n} P(A_{i_1}\cap A_{i_2}) \\
                                        &+ \sum_{1\leq i_1 \leq i_2 \leq i_3 \leq n} P(A_{i_1} \cap A_{i_2} \cap A_{i_3}) \\
                                        &- \sum_{1\leq i_1 \leq i_2 \leq i_3 \leq i_4 \leq n} P(A_{i_1} \cap A_{i_2} \cap A_{i_3} \cap A_{i_4}) \\
                                        &+ \dots + (-1)^{n+1} P(A_1 \cap \dots \cap A_n) \\
                                        &= \sum^n_{k=1} (-1)^{k+1} \sum_{1\leq i_1 < \dots < i_k \leq n} P(A_{i_1} \cap \dots \cap A_{i_k})
        \end{axiom}

    \subsection{Random Variables: A First Look}             % 1.5


    \subsection{Finer Points}                               % 1.6