\section{Introduction}
\noindent \textbf{Machine Learning (ML):} the field of study that gives computers the ability to learn
without being explicitly programmed \\
\noindent \textbf{Learning Problems:} a computer program is said to learn from experience E with respect to some
task T and some performance measure P, if its performance on T, as measured by P, improves with
experience E. \\

\noindent \textit{Example: Suppose your email program watches which emails you do or do not mark as spam, and based on
that learns how to better filter spam. Identify the experience E, task T, and performance measure P
in this scenario.}
\noindent \textbf{T:} Classifying emails as spam or not spam. \\
\noindent \textbf{E:} Watching you label emails as spam or not spam. \\
\noindent \textbf{P:} The number (or fraction) of emails correctly classified as spam/not spam. \\

\noindent \textbf{Supervised Learning:} we give the computer a dataset and the right/wrong answers
and then attempt to produce more correct answers. \\
\noindent \textbf{Unsupervised Learning:} we approach problems with little to no idea of what our
results should look like and we derive structure from data where we don't necessarily know the effect
of the variables; we let the computer learn itself, e.g. organizing computing clusters, social network
analysis, market segmentation, astronomical data analysis. \\

\noindent \textbf{Clustering:} finding natural groups in the feature space of input data \\
\noindent \textbf{Regression:} attempting to predict continuous values \\
\noindent \textbf{Classification:} attemping to predict discrete-valued output or map input variables
into discrete categories, e.g. Benign vs. Malignant Tumor \\



\pagebreak
\section{Univariate Linear Regression}
\noindent \textbf{Training Set:} the dataset used to train a model \\
$\bm{m}$: number of training examples \\
$\bm{x}$'s: input variables/features \\
$\bm{y}$'s: output variables/features \\
$\bm{(x,y)}$: one training example \\
$\bm{(x^{(i)}, y^{(i)})}$: the $i$th training example \\
\textbf{Hypothesis Function ($h:x\to y,h(x)=h_\theta (x)=\theta_0+\theta_1 x$)}: given a training set,
we want to choose \textbf{parameters ($\theta_0$ and $\theta_1$)}such that the mean distance between
$h(x)$ and the training examples are as close to the true mean distance:

\begin{figure}[hbt!]
    \centering
    \includegraphics[scale=0.2]{Resources/Linear_Regression.png}
    \caption*{\textbf{Linear Regression}}
\end{figure}

\begin{figure}[hbt!]
    \centering
    \includegraphics[scale=0.75]{Resources/Hypothesis_Function.png}
    \caption*{\textbf{Hypothesis Function}}
\end{figure}

\noindent We can measure the error of a hypothesis function by using a \textbf{cost function} $J$,
given by

\begin{align*}
    J(\theta_0,\theta_1)    &= \frac{1}{2m}\sum^m_{i=1}(h_\theta(x_i)-y_i)^2
\end{align*}

\noindent This function is really just the average of the differences between the produced outputs
with the hypothesis function being tested, and the actual outputs. The cost function is also referred
to as the squared error function. We half the mean in the cost function out of convenience for
computing the gradient descent, since the derivative term of the square function will cancel out the
half. \\

\noindent The contour plot display of the cost function for a particular hypothesis function:

\begin{figure}[hbt!]
    \centering
    \includegraphics[scale=0.75]{Resources/Contour.PNG}
\end{figure}

\noindent \textbf{Gradient Descent:} an optimization algorithm used to minimize some function, e.g. the
cost function, by iteratively moving in the direction of the steepest descent as defined by the negative
of the gradient. \\

\noindent Putting $\theta_0$ on the $x$-axis and $\theta_1$ on the $y$-axis, with the cost function
on the $z$-axis, the points on the graph are the outputs of our cost function. We know we have succeeded
in finding parameters for $J(\theta_0,\theta_1)$ that minimize the cost when our gradient descent leads
us to the blue area of the graph (lowest $z$-value). The red arrows on the graph show the minimum points
on the graph.

\begin{figure}[hbt!]
    \centering
    \includegraphics[scale=0.75]{Resources/Gradient_Descent}
\end{figure}

\noindent We implement this algorithm by taking the derivative of the cost function. This gives us the
slope of the cost function at a point, and a direction to move towards. We then iteratively step down
the cost function in the direction of the steepest descent. The size of each step is determined by the
\textbf{learning rate}, the parameter $\alpha$. Note that $\alpha$ is fixed and its value need not variate
because the cost function is parabolic, hence as we approach the minimum, the derivative decreases in
magnitude and flattens out. \\

\noindent We want to choose a value of $\alpha$ that is not too small or too large:

\begin{figure}[hbt!]
    \centering
    \includegraphics[scale=0.5]{Resources/Learning_Rate.PNG}
\end{figure}

\noindent Then our implementation of gradient descent is given by repeating the following equation until
convergence.

\begin{equation*}
    \theta_j := \theta_j-\alpha\frac{\partial}{\partial \theta_j}J(\theta_0,\theta_1)
\end{equation*}

\noindent where $j=0,1$ represents the index and "$:=$" is the assignment operator. \\

\noindent Note that the gradient descent equations for $\theta_0$ and $\theta_1$ should be updated
simultaneously, such that updating of $\theta_0$ does not interfere with the accurate updating of $\theta_1$:

\begin{figure}[hbt!]
    \centering
    \includegraphics[scale=0.75]{Resources/Simultaneous_Update}
\end{figure}

\noindent The current gradient descent algorithm we are using is called \textbf{batch gradient descent}
because it accounts for every training example to determine the descent steps. For a single training
example,

\begin{align*}
    \frac{\partial}{\partial\theta_j}J(\theta)  &= \frac{\partial}{\partial\theta_j}\frac{1}{2}(h_\theta(x)-y)^2 \\
    &= 2\cdot\frac{1}{2}(h_\theta(x)-y)\cdot
    \frac{\partial}{\partial\theta_j}(h_\theta(x)-y) \\
    &= (h_\theta(x)-y)\cdot\frac{\partial}{\partial\theta_j}
    \left(\sum^n_{i=0}\theta_i x_i-y\right) \\
    &= (h_\theta(x)-y)x_j
\end{align*}

\noindent Hence, we repeat the following equations until convergence.

\begin{align*}
    \theta_0 &:= \theta_0-\alpha\frac{1}{m}\sum^m_{i=1}(h_\theta(x_i)-y_i) \\
    \theta_1 &:= \theta_1-\alpha\frac{1}{m}\sum^m_{i=1}(h_\theta(x_i)-y)x_i
\end{align*}

\noindent While gradient descent can be susceptible to local minima, simple optimization problems for
linear regression only have one global, and no other local or optima. Thus, gradient descent will always
converge here to the global minimum. Assuming we have a well-chosen $\alpha$, we see that $J$ is indeed
a convex quadratic function, and the below contour plot depicts the trajectory of an implementation of
gradient descent to minimize a quadratic function. The points start from the warm outermost colors and
move towards the cooler innermost colors, until they converge at the global minimum of the cost function.

\begin{figure}[hbt!]
    \centering
    \includegraphics[scale=0.75]{Resources/Contour2}
\end{figure}



\pagebreak
\section{Linear Algebra Review}
\textbf{Matrix:} a 2D array \\
$\bullet$ The dimension of a matrix is given by "number of rows $\times$ number of columns" \\
$\bullet$ Dimension may be written $\mathbb{R}^{4\times 2}$, representing a $4\times2$ matrix of the
real-valued set $\mathbb{R}$ \\
$\bullet$ In the declaration below, $A_{iy}=$ "$i,j$ entry" in the $i$th row, $j$th column

\begin{equation*}
    A = \begin{bmatrix}
            1402 & 191 \\
            1371 & 821 \\
            949  & 1437 \\
            147  & 1448
    \end{bmatrix}
\end{equation*}

\noindent \textbf{Vector:} A $n\times1$ matrix \\
$\bullet$ Can be 1-indexed or 0-indexed, where 1-indexed is more conventional \\
$\bullet$ It is conventional to name matrices with capital letters and vectors with lowercase letters \\
$\bullet$ In the declaration below, $y_i=i$th element of the given 4D Vector, $\mathbb{R}^4$

\begin{equation*}
    y = \begin{bmatrix}
            460 \\
            232 \\
            315 \\
            178
    \end{bmatrix}
\end{equation*}

\noindent \textbf{Matrix Addition:} a defined matrix sum must have two addends with the same dimensions.

\begin{equation*}
    \begin{bmatrix}
        a_1 & d_1 \\
        b_1 & e_1 \\
        c_1 & f_1
    \end{bmatrix}
    +
    \begin{bmatrix}
        a_2 & d_2 \\
        b_2 & e_2 \\
        c_2 & f_2
    \end{bmatrix}
    =
    \begin{bmatrix}
        a_1 + a_2   &= d_1 + d_2 \\
        b_1 + b_2   &= e_1 + e_2 \\
        c_1 + c_2   &= f_1 + f_2
    \end{bmatrix}
\end{equation*}

\noindent \textbf{Scalar Matrix Multiplication:}

\begin{equation*}
    C   \begin{bmatrix}
            a & d \\
            b & e \\
            c & f
    \end{bmatrix}
    =
    \begin{bmatrix}
        Ca & Cd \\
        Cb & Ce \\
        Cc & Cf
    \end{bmatrix}
\end{equation*}

\noindent Division example:

\begin{equation*}
    \begin{bmatrix}
        4 & 0 \\
        6 & 3
    \end{bmatrix}
    /4
    =
    \frac{1}{4}
    \begin{bmatrix}
        4 & 0 \\
        6 & 3
    \end{bmatrix}
    =
    \begin{bmatrix}
        1   & 0 \\
        3/2 & 3/4
    \end{bmatrix}
\end{equation*}

\noindent \textbf{Matrix-Vector Multiplication:} if $A$ is a $m\times n$ matrix then the product $Ax$
is defined by $n\times 1$ column vectors $x$. If $Ax=b$ then $b$ is a $m\times 1$ column vector.

\begin{figure}[hbt!]
    \centering
    \includegraphics[scale=0.75]{Resources/Matrix_Vector_Mult}
\end{figure}

\noindent Example:

\begin{equation*}
    \begin{bmatrix}
        1 & -1 & 2 \\
        0 & -3 & 1
    \end{bmatrix}
    \begin{bmatrix}
        2 \\
        1 \\
        0
    \end{bmatrix}
    =
    \begin{bmatrix}
        2\cdot 1    & -1\cdot 1 & 0\cdot 2 \\
        2\cdot 0    & -1\cdot 3 & 0\cdot 1
    \end{bmatrix}
    =
    \begin{bmatrix}
        1 \\
        -3
    \end{bmatrix}
\end{equation*}

\noindent \textbf{Matrix-Matrix Multiplication to Combine Hypotheses:} If we wanted to make a
hypothesis from 3 hypotheses we could represent the hypotheses by matrices and the dataset by another
matrix and multiply them to get the product matrix which would be the combined hypothesis:

\begin{figure}[hbt!]
    \centering
    \includegraphics[scale=0.4]{Resources/Hypothesis_Combination}
\end{figure}

\pagebreak
\noindent \textbf{Properties of Matrix-Matrix Multiplication:} \\
$\bullet$ In general, $A\times B\not = B\times A$ (not commutative) \\
$\bullet$ $A\times(B\times C)=(A\times B)\times C$ (is associative) \\

\noindent \textbf{Identity Matrix:} a special square matrix that has 1's along the main diagonal
(upper left to lower right) and 0's for all other elements \\
$\bullet$ Denoted $I$ or $I_{n\timesm}$ \\
$\bullet$ For any matrix $A,A_{m\times n}\cdot I_{n\times n}=I_{m\times m}\cdot A_{m\times n}=
A_{m\times n}$ (the subscripts are usually omitted) \\
$\bullet$ $A\timesB=B\timesA$ when $B=I$ (matrices are commutative when at least one of them is the
identity matrix) \\

\noindent Example of an identity matrix:

\begin{equation*}
    I_3 =
    \begin{bmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0 \\
        0 & 0 & 1
    \end{bmatrix}
\end{equation*}

\noindent Informally,

\begin{figure}[hbt!]
    \centering
    \includegraphics[scale=1.2]{Resources/Identity2}
\end{figure}

\noindent \textbf{Matrix Inverse:} If $A$ is a $m\times m$ matrix, and if it has an inverse
(only square matrices have inverses), then $A(A^{-1})=A^{-1}A=I$ \\

\noindent \textbf{Matrix Transpose:} If $A$ is a $m\times n$ matrix, $B=A^T$ then $B$ is a $n\times m$
matrix, and $B_{ij}=A_{ij}$ \\
$\bullet$ Basically, the rows of $A$ become the columns of $B$ and the columns of $A$ become the
rows of $B$

\begin{equation*}
    A =
    \begin{bmatrix}
        1 & 2 & 0 \\
        0 & 5 & 6 \\
        7 & 0 & 9
    \end{bmatrix}
    , A^T =
    \begin{bmatrix}
        1 & 0 & 7 \\
        2 & 5 & 0 \\
        0 & 6 & 9
    \end{bmatrix}
\end{equation*}



\pagebreak
\section{Multivariate Linear Regression}
\textbf{Notation:}
$\bm{x_j^{(i)}}=$ value of features $j$ in $i$th training example \\
$\bm{x^{(i)}}=$ input (features) of $i$th training example \\
$\bm{m}=$ the number of training examples \\
$\bm{n}=$ the number of features \\

\noindent We can extend the univarate hypothesis function, $h_\theta(x)=\theta_0+\theta_1 x $ to
multiple variables like so:

\begin{equation*}
    h_{\theta}(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_3+\dots+\theta_n x_n
\end{equation*}

\noindent We can represent the \textbf{multivariate hypothesis function} by defining $x_0=1$ and
writing the $x$'s and $\theta$'s as parameter vectors:

\begin{equation*}
    x = \begin{bmatrix}
            x_0 \\
            x_1 \\
            x_2 \\
            \vdots \\
            x_n
    \end{bmatrix}
    \in\mathbb{R}^{n+1}
    ,\theta = \begin{bmatrix}
                  \theta_0 \\
                  \theta_1 \\
                  \theta_2 \\
                  \vdots \\
                  \theta_n
    \end{bmatrix}
    \in\mathbb{R}^{n+1}
\end{equation*}

\noindent Then

\begin{align*}
    h_{\theta}(x)   &= \theta_0 x_0 + \theta_1 x_1 + \dots + \theta_n x_n \\
    &= \theta^T x
\end{align*}

\noindent where $\theta^T=[\theta_0,\theta_1,\dots,\theta_n]$ and
$x=\begin{bmatrix}x_0\\x_1\\\vdots\\\x_n\end{bmatrix}$. Together, $\theta^T x$ is read
"theta transpose $x$". \\

\noindent \textbf{Multivariate Gradient Descent Equation:}

\begin{align*}
    \theta_j &:= \theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta) \\
    &:= \theta_j-\alpha\frac{1}{m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}
\end{align*}

\noindent where we simulatenously update $\theta_j$ for $j=0,\dots,n$. Note that the parameter of $J$,
$\theta$, represents a vector containing all the $\theta$'s from $\theta_0$ to $\theta_n$. \\

\noindent We can speed up gradient descent by having our input values in roughly the same range. This
is because $\theta$ descends faster on small ranges and slowly on large ranges so it will oscillate
inefficiently down to the minimum of the cost function when the variables are uneven. Ideally, we want
$-1\leq x_{(i)}\leq 1$ or $-0.5\leq x_{(i)}\leq 0.5$. If the range is too large, say, $-3\leq x\leq 3$
or too small, like $-\frac{1}{3}\leq x\leq\frac{1}{3}$, then the contours of our cost function will
be really narrow and gradient descent will be more inefficient.

\begin{figure}[hbt!]
    \centering
    \includegraphics[scale=0.4]{Resources/Contour3}
\end{figure}

\noindent One way to prevent gradient descent inefficiency is by \textbf{feature scaling}, which
involves dividing the input values by the range of the input variable, resulting in a new range of
simply 1. The following equation demonstrates how we rescale a value of a features.

\begin{equation*}
    x_{\text{new}} = \frac{x_i-\text{min}(x)}{\text{max}(x)-\text{min}(x)}
\end{equation*}

\noindent \textbf{Mean Normalization} is a second way we might rescale features, involving subtracting
the average value for an input variable from the values for that input variable which result in a new
average value for the input variable of simply 0. We adjust input values by the following formula.

\begin{equation*}
    x_i := \frac{x_i - \mu_i}{s_i}
\end{equation*}

\noindent where $\mu_i$ is the mean of the feature values and $s_i$ is the range of values or the
standard deviation. For example, if $x_i$ represents housing prices with a range of 100 to 2000
and a mean value of 1000, then $x_i := \frac{\text{price}-1000}{1900}$.

\pagebreak
\noindent We can debug gradient descent by making a plot with the number of iterations on the $x$-axis.
Then plotting the cost function over the number of iterations of gradient descent, if $J(\theta)$ ever
increases, then it is likely that we need to decrease the value of $\alpha$. If $\alpha$ is too large,
then the cost function may not decrease on every iteration and may diverge. However, if $\alpha$ is
too small, the convergence may be too slow. \\

\noindent Common plots that show we need a smaller learning rate include the two figures below.

\begin{figure}[hbt!]
    \centering
    \begin{subfigure}[b]{.45\linewidth}
        \includegraphics[scale=1]{Resources/Gradient_Descent_Debug}
    \end{subfigure}
    \begin{subfigure}[b]{.45\linewidth}
        \includegraphics[scale=0.8]{Resources/Gradient_Descent_Debug2}
    \end{subfigure}
\end{figure}

\noindent We can change the behavior of a hypothesis function by making it a different type of
function, including quadratic, cubic, and radical if it fits the data better. If our hypothesis
function is $h_\theta (x)=\theta_0+\theta_1 x_1$ then we can create additional features based on $x_1$
to get the square root function $h_\theta(x)=\theta_0 +\theta_1 x_1+\theta_2\sqrt{x_1}$ or the cubic
function $h_\theta(x)=\theta_0 + \theta_1 x_1 + \theta_2 x_1^2 + \theta_3 x_1^3$.



\pagebreak
\section{Computing Parameters Analytically}
\textbf{Normal Equation:} an analytical approach to Linear Regression with a Least Square Cost Function \\
$\bullet$ Allows us to directly find the parameters $\theta$ without using gradient descent. \\

\noindent \textbf{A Derivation of the Normal Equation:} \\
We have our hypothesis function $h(\theta)=\theta^T x$ and cost function $J(\theta)=\frac{1}{2m}
\sum^m_{i=1}[h_\theta (x^{(i)})-y^{(i)}]^2$. Let us create a matrix $X$ such that

\begin{align*}
    X =
    \begin{bmatrix}
        x^1 \\
        x^2 \\
        \vdots \\
        x^i
    \end{bmatrix}
\end{align*}

\noindent where $X$ is a $m\times 1$ matrix and each row is the $i$th training example. Now we can use
$X$'s definition to rewrite the cost function $J(\theta)$, ignoring the $\frac{1}{2m}$ because we're
computing the derivative to zero so it doesn't matter. Then

\begin{align*}
    J(\theta) &= ((X\theta)^T-y^T)(X\theta-y) \\
    &= (X\theta)^T X\theta - (X\theta)^T y - y^T(X\theta) + y^T y \\
    &= \theta^T X^T X\theta-2(X\theta)^T y+y^T y
\end{align*}

\noindent Note that both $X\theta$ and $y$ are vectors, hence, they have the associative property of
multiplication and we were able to simplify down to the above equation. Since we want to minimize the
cost function $J(\theta)$, let's set the partial derivative to 0.

\begin{align*}
    \frac{\partial J(\theta)}{\partial\theta}  = 0
\end{align*}

\noindent We can ignore the last term of $J(\theta)$ because its behavior doesn't depend on the variable
of interest. Let us differentiate the second term of $J(\theta)$, which we will refer to as

\begin{equation*}
    P(\theta) = 2(X\theta)^T y
\end{equation*}

\noindent Recall that $\theta$ is a vector of $n$ components, $y$ is a vector of $m$ components, and
$X$ is a $m\times n$ matrix.

\noindent Then the matrix expansion of $P(\theta)$ is

\begin{align*}
    P(\theta) = 2
    \left[
    \begin{pmatrix}
        x_{11} & x_{12} & \dots & x_{1n} \\
        x_{21} & \dots  & \dots & x_{2n} \\
        \vdots & \vdots & \vdots & \vdots \\
        x_{m1} & \dots & \dots & x_{mn}
    \end{pmatrix}
    \begin{pmatrix}
        \theta_1 \\
        \theta_2 \\
        \vdots \\
        \theta_n
    \end{pmatrix}
    \right]^T
    \begin{pmatrix}
        y_1 \\
        y_2 \\
        \vdots \\
        y_m
    \end{pmatrix}
\end{align*}

\noindent Then

\begin{align*}
    P(x) &= 2
    \left[
    \begin{pmatrix}
        x_{11}\theta_1 + \dots + x_{1n}\theta_n \\
        x_{21}\theta_1 + \dots + x_{2n}\theta_n \\
        \vdots \\
        x_{m1}\theta_1 + \dots + x_{mn}\theta_n
    \end{pmatrix}
    \right]^T
    \begin{pmatrix}
        y_1 \\
        y_2 \\
        \vdots \\
        y_m
    \end{pmatrix} \\
    &= 2(x_{11}\theta_1+\dots+x_{1n}\theta_n)y_1 + 2(x_{21}\theta_1+\dots+x_{2n}\theta_n)y_2
    + \dots + 2(x_{m1}\theta_1+\dots+x_{mn}\theta_n)y_m \\
    &= 2\sum^m_{i=1}y_i(x_{i1}\theta_1 +\dots+ x_{in}\theta_n) \\
    &= 2\sum^m_{i=1}y_i \sum^n_{j=1} x_{ij}\theta_j
\end{align*}

\noindent and

\begin{align*}
    \frac{\partial P}{\partial \theta_1}    &= 2(x_{11}y_1+\dots+x_{m1}y_m) \\
    \frac{\partial P}{\partial \theta_2}    &= 2(x_{12}y_1+\dots+x_{m2}y_m) \\
    \frac{\partial P}{\partial \theta_n}    &= 2(x_{1n}y_1+\dots+x_{mn}y_m)
\end{align*}

\noindent Since $\frac{\partial P}{\partial\theta}$ can be thought of as a vector of $n$ components,
we can rewrite these equations using a matrix-by-vector multiplication, effectively getting

\begin{equation*}
    \frac{\partial P}{\partial\theta} = 2X^T y
\end{equation*}

\noindent We also now know that

\begin{equation*}
    \frac{\partial}{\partial\theta} (X\theta)^T y = X^T y
\end{equation*}

\noindent Going back to $J(\theta) = \theta^T X^T X\theta-2(X\theta)^T y+y^T y$, let us define the
first term as

\begin{align*}
    Q(\theta) = \theta^T X^T X\theta
\end{align*}

\noindent We expand and transpose $Q(\theta)$ such that

\begin{align*}
    Q(\theta) = (\theta_1\dots\theta_n)
    \begin{pmatrix}
        x_{11} & x_{21} & \dots & x_{m1} \\
        x_{12} & \dots  & \dots & x_{m2} \\
        \vdots & \vdots & \vdots & \vdots \\
        x_{1n} & \dots  & \dots & x_{mn}
    \end{pmatrix}
    \begin{pmatrix}
        x_{11} & x_{12} & \dots & x_{1n} \\
        x_{21} & \dots  & \dots & x_{2n} \\
        \vdots & \vdots & \vdots & \vdots \\
        x_{m1} & \dots  & \dots & x_{mn}
    \end{pmatrix}
    \begin{pmatrix}
        \theta_1 \\
        \theta_2 \\
        \vdots \\
        \theta_n
    \end{pmatrix}
\end{align*}

\noindent Multiplying the two largest matrices together, we have a "$X$-squared matrix" which is
$n\times n$. The rows $r$ and columns $c$ satisfy the declaration

\begin{equation*}
    X^2_{rc} = \sum^m_{i=1} x_{ir}x_{ic}
\end{equation*}

\noindent We multiply $X^2_{rc}$ by the $\theta$ vector to get

\begin{align*}
    Q(\theta) &= (\theta_1\dots\theta_n)
    \begin{pmatrix}
        x^2_{11}\theta_1 + \dots + x^2_{1n}\theta_n \\
        x^2_{21}\theta_1 + \dots + x^2_{2n}\theta_n \\
        x^2_{n1}\theta_1 + \dots + x^2_{nn}\theta_n \\
    \end{pmatrix} \\
    &= \theta_1(X^2_{11}\theta_1+\dots+X^2_{1n}\theta_n) + \theta_2(X^2_{21}\theta_1+\dots+X^2_{2n}\theta_n)
    +\dots+\theta_n(X^2{n1}\theta_1+\dots+X^2_{nn}\theta_n)
\end{align*}

\noindent We have

\begin{align*}
    \frac{\partial Q}{\partial \theta_1} = (2\theta_1 x^2_{11}_\theta_2 X^2_{12}+\dots+
    \theta_n X^2_{21}+\dots+\theta_n X^2{n1})
\end{align*}

\noindent Since $X^2$ is symmetric, we know that $X^2_{12} = X^2_{21}$, etc. Hence,

\begin{align*}
    \frac{\partial Q}{\partial \theta_1} &= 2\theta_1 X^2_{11}+2\theta_2 X^2_{12}+\dots+2\theta_n X^2_{1n} \\
    \frac{\partial Q}{\partial \theta} &= 2X^2\theta \\
    \frac{\partial Q}{\partial \theta} &= 2X^T X\theta \\
    \frac{\partial J}{\partial \theta} &= \frac{\partial Q}{\partial\theta}-\frac{\partial P}{\partial\theta} \\
    &= 2X^T X\theta - 2X^T y
\end{align*}

\noindent We want to set the partial derivative to 0 in order to find the values of $\theta$ which
minimize the cost function $J(\theta)$:

\begin{align*}
    2X^T X\theta - 2X^T y &= 0 \\
    X^T X\theta &= X^T y
\end{align*}

\noindent Assume the matrix $X^T X$ is invertible; we can multiply both sides of the above equation
by $(X^T X)^{-1}$. Then we get the \textbf{normal equation,}

\begin{equation*}
    \theta = (X^T X)^{-1} X^T y
\end{equation*}

\noindent Below is an example of using the normal equation to quickly calculate the desired value of
$\theta$.

\begin{figure}[hbt!]
    \centering
    \includegraphics[scale=0.5]{Resources/Norm_Equation_Example}
\end{figure}

\pagebreak

\begin{figure}[hbt!]
    \centering
    \includegraphics[scale=0.4]{Resources/Norm_Equation_Example2}
\end{figure}

\begin{figure}[hbt!]
    \centering
    \includegraphics[scale=0.4]{Resources/Norm_Equation_Example3}
\end{figure}

\noindent Both gradient descent and using the normal equation have their advantages and disadvantages.
As a general rule of thumb,

\begin{center}
    \begin{tabular}{|c|c|}
        \hline
        \textbf{Gradient Descent}       & \textbf{Normal Equation} \\
        \hline
        Need to choose $\alpha$         & No need to choose $\alpha$ \\
        \hline
        Needs many iterations           & No need to iterate \\
        \hline
        $O(kn^2)$                       & $O(n^3)$, need to calculate inverse of $X^T X$ \\
        \hline
        Works well when $n$ is large    & Slow if $n$ is very large \\
        \hline
    \end{tabular}
\end{center}

\noindent In practice, it might be a good time to start using an iterative process when $n>10,000$. \\

\noindent In this class, we want to use the Octave "pinv" function instead of "inv", which will give
a value of $\theta$ even if $X^T X$ is non-invertible. If $X^T X$ is non-invertible, common causes
include redundant features, where two features are very closely related (linearly dependent), or
too many features (e.g. $m\leq n$).



\pagebreak
\section{Logistic Regression}
Because classification is not actually a linear function, it is often unwise to use linear regression
to map predictions greater than 0.5 as a 1 and less than 0.5 as a 0. \textbf{Logistic Regression} is
a classification algorithm, \textbf{binary logistic regression} being a scenario where the output, $y$,
can take on only two values, 0 and 1. Conventionally, the 0 is the "negative" class for which the
output does not show signs of a particular feature, and the 1 is the "positive" class. For example, if
we are trying to build a spam filter for email, then $x^{(i)}$ may be some features of an email, and
$y$ may be 1 if it is a spam mail, and 0 otherwise. Then $y\in \{0,1\}$. Given $x^{(i)}$, the
corresponding $y^{(i)}$ is also called the \textbf{label} for the training example. \\

\noindent We use the \textbf{sigmoid} function, $g(z)$ for logistic regression:

\begin{align*}
    \text{Desired } & 0\leq h_{\theta}(x)\leq 1 \\
    h_{\theta}(x)  &= g(\theta^T x) \\
    z              &= \theta^T x \\
    g(z)           &= \frac{1}{1+e^{-z}}
\end{align*}

\noindent The graph for the sigmoid function looks like this:

\begin{center}
    \begin{tikzpicture}
        \begin{axis}[
        xmin = -7.5,
        xmax = 7.5,
        ymin = -0.1,
        ymax = 1.1,
        axis lines = center,
        xlabel = $x$,
        ylabel = $y$,
        xtick={-7.5,-5,-2.5,0,2.5,5,7.5},
        ytick={0,0.5,1},
        height=4cm,
        width=15cm
        ]
            \addplot[
            domain = -7.5:7.5,
            range = 0:1
            ]
            {1/(1+exp(-x))};
        \end{axis}
    \end{tikzpicture}
\end{center}

\noindent $h_{\theta}(x)$ gives us the probabilities of the outputs such that

\begin{align*}
    h_{\theta}(x)   &= P(y=1|x;\theta) = 1 - P(y=0|x;\theta) \\
    & \text{and} \\
    1               &= P(y=0|x;\theta) + P(y=1|x;\theta)
\end{align*}

\noindent In order to get a 0 or 1 classification, we can translate the hypothesis function output as
follows:

\begin{align*}
    h_{\theta}(x) \geq 0.5 &\to y=1 \\
    h_{\theta}(x) < 0.5    &\to y=0 \\
    \implies \\
    \theta^T x \geq 0  &\to y=1 \\
    \theta^T x < 0     &\to y=0
\end{align*}

\noindent Say we have a dataset such that our training examples can be plotted with red X's
representing positive outputs and blue circle's representing negative outputs, like so:

\begin{figure}[hbt!]
    \centering
    \includegraphics{Resources/Decision_Boundary}
\end{figure}

\noindent Then we can add two more training terms to the hypothesis function, which becomes

\begin{align*}
    h_{\theta}(x)   &= g(\theta_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_1^2 + \theta_4 x_2^2) \\
    &= g(-1 + 0x_1 + 0x_2 + 1x_1^2 + 1x_2^2)
\end{align*}

\noindent The $\theta$ parameter vector is then

\begin{equation*}
    \theta =
    \begin{bmatrix}
        -1 \\
        0 \\
        0 \\
        1 \\
        1
    \end{bmatrix}
\end{equation*}

\noindent Thus, by looking at the values of the parameter vector $\theta$, we can predict $y=1$ if
$-1+x_1^2 + x_2^2\geq 0$, or if $x_1^2+x_2^2\geq 1$. Notice that $x_1^2+x_2^2=1$ forms the equation
for a circle. This circle with radius 1 centered at $(0,0)$ is our \textbf{decision boundary}, as
graphed below. Hence, everything inside the circle may be predicted as $y=0$, whereas everything
outside the circle may be predicted as $y=1$.

\begin{figure}[hbt!]
    \centering
    \includegraphics[scale=0.5]{Resources/Decision_Boundary2}
\end{figure} \\\\

\noindent \textbf{Cost Function for Logistic Regression:}

\begin{align*}
    J(\theta)   &= \frac{1}{m}\sum^m_{i=1}\text{ Cost}(h_{\theta}(x^{(i)}),y^{(i)}) \\
    \text{Cost}(h_{\theta}(x),y) &=
    \begin{cases}
        -\log{(h_{\theta}(x))}   & \text{ if } y=1 \\
        -\log{(1-h_{\theta}(x))} & \text{ if } y=0
    \end{cases} \\
    J(\theta)   &= -\frac{1}{m}\left[
    \sum^m_{i=1}y^{(i)}\log({h_\theta (x^{(i)})}) + (1-y^{(i)})\log{(1-h_\theta (x^{(i)}))}
    \right]
\end{align*}

\noindent Vectorized, this is implemented as follows:

\begin{align*}
    h   &= g(X\theta) \\
    J(\theta) &= \frac{1}{m}\cdot\left(-y^T \log{(h)}-(1-y)^T \log{(1-h)}\right)
\end{align*}

\begin{figure}[hbt!]
    \centering
    \begin{subfigure}[b]{.45\linewidth}
        \includegraphics[scale=0.6]{Resources/Log_Model}
    \end{subfigure}
    \begin{subfigure}[b]{.45\linewidth}
        \includegraphics[scale=0.6]{Resources/Log_Model2}
    \end{subfigure}
\end{figure}

\noindent \textbf{Properties of the Logistic Cost Function:}

\begin{align*}
    \text{Cost }(h_{\theta}(x),y)   &= 0 \text{ if } h_{\theta}(x) = y \\
    \text{Cost }(h_{\theta}(x),y)   &\to\infty \text{ if } y=0 \text{ and } h_{\theta}(x)\to 1 \\
    \text{Cost }(h_{\theta}(x),y)   &\to\infty \text{ if } y=1 \text{ and } h_{\theta}(x)\to 0
\end{align*}

\noindent \textbf{Gradient Descent on Logistic Models:} \\

\noindent We want to minimize $J(\theta)$ by repeating the following equation.

\begin{align*}
    \thea_j     &:= \theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta) \\
    \theta_j    &:= \theta_j-\alpha\sum^m_{i=1} (h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)} \\
    \text{where} \\
    h_{\theta}(x^{(i)}) &= \frac{1}{1+e^{-\theta^T x}}
\end{align*}

\noindent Notice how this equation is the same as the one for linear models, except for the value of
$h_{\theta}(x^{(i)})$. A vectorized implementation of logistic gradient descent is below.

\begin{equation*}
    \theta := \theta - \frac{\alpha}{m} X^T (g(X\theta)-\overrightarrow{y})
\end{equation*}

\pagebreak

\noindent \textbf{Optimization Algorithm:} \\
Given $\theta$, we have code that can compute \\
$\bullet$ $J(\theta)$ \\
$\bullet$ $\frac{\partial}{\partial\theta_j}J(\theta)$ for $j=0,1,\dots,n$

\noindent Optimization Algorithms: \\
$\bullet$ Gradient Descent \\
$\bullet$ Conjugate Gradient \\\
$\bullet$ BFGS \\
$\bullet$ L-BFGS \\
$\bullet$ Advantages: No need to pick $\alpha$, often faster than gradient descent \\
$\bullet$ Disadvantages: More complex \\

\noindent \textbf{One vs. All Multiclass Classification:} \\
Recall how binary classification has output $y=\{0,1\}$. In multiclass classification, $y=\{0,1,\dots,n\}$.
We can divide our problem by repeatedly applying binary logistic regression to each output case and
then use the hypothesis that returned the highest value as our prediction. The following are some
algebraic and visual representations of this.

\begin{align*}
    y                   & \in\{0,1,\dots,n\} \\
    h_{\theta}^{(0)}(x) &= P(y=0|x;\theta) \\
    h_{\theta}^{(1)}(x) &= P(y=1|x;\theta) \\
    \vdots \\
    h_{\theta}^{(n)}(x) &= P(y=n|x;\theta) \\
    \text{prediction }  &= \text{ max}(h_{\theta}^{(i)}(x))
\end{align*}

\begin{figure}[hbt!]
    \centering
    \includegraphics[scale=0.9]{Resources/Multiclass_Classification}
\end{figure}



\pagebreak
\section{Regularization}
Consider the challenge of predicting $y$ from $x\in\mathbb{R}$. The leftmost figure shows the result
of using linear regression on a dataset. If we added a second feature to the regression, making it
$y=\theta_0+\theta_1 x+\theta_2 x^2$, then we get a better fit (the middle figure). If we continue
adding features such that we have the 5th order polynomial $y=\sum^5_{j=0}\theta_j x^j$ (displayed
on the right). Notice that the left figure has high bias and we call this \textbf{underfitting}.
This is usually caused by having too simple of a regression curve, or not using enough features.
At the other extreme, the right figure has high variance caused by too many features and is referred
to as \textbf{overfitting}. The hypothesis function for overfitting fits the available data but
does not generalize well to predict new data. \\

\noindent \textbf{Ways to Address Overfitting:} \\
1) Reduce the number of features: \\
$\bullet$ Manually select which features to keep \\
$\bullet$ Use a model selection algorithm \\
2) Regularization \\
$\bullet$ Keep all the features, but reduce the magnitude of parameters $\theta_j$ \\
$\bullet$ Works well when we have a lot of slightly useful features \\

\noindent Let's say we wanted to make the function $\theta_0 + \theta_1 x + \theta_2 x^2 + \theta_3 x^3
+ \theta_4 x^4$. We will want to reduce the influence of $\theta_3 x^3$ and $\theta_4 x^4$. We can
modify our cost function like so:

\begin{align*}
    \text{min }_{\theta} \frac{1}{2m} \sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2 + 1000\theta^2_3 +
    1000\theta^2_4
\end{align*}

\noindent We have added two extra terms to inflate the cost of $\theta_3$ and $\theta_4$. In order for
the cost function to get close to zero we will have to reduce the values of $\theta_3$ and $\theta_4$.
We can see that the new hypothesis (pink curve) looks like a quadratic function but fits the data
better because of the extra small terms $\theta_3 x^3$ and $\theta_4 x^4$.

\begin{figure}[hbt!]
    \centering
    \includegraphics[scale=0.75]{Resources/Regularization}
\end{figure}

\noindent We can also regularize all of the $\theta$ parameters in a single summation as:

\begin{align*}
    \text{min }_{\theta}\frac{1}{2m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2
    \color{blue} + \lambda \sum^n_{j=1}\theta^2_j \color{black}
\end{align*}

\noindent where $\lambda$ is the \textbf{regularization parameter}, determining to what magnitude the
costs of our $\theta$ parameters are inflated. By convention, the $\sum^n_{j=1}\theta^2_j=\theta_1+
\theta_2+\dots+\theta_n$. If $\lambda$ is chosen to be too large, the function may be smoothed out too
much and cause underfitting. Conversely, if $\lambda$ is chosen to be too small, the function may not
be smoothed out enough and cause overfitting. \\

\noindent \textbf{Regularized Linear Regression:} \\
We wish to repeat

\begin{align*}
    \theta_0    &:= \theta_0-\alpha\frac{1}{m}\sum^m_{i=1} (h_\theta(x^{(i)})-y^{(i)})x_0^{(i)} \\
    \theta_j    &:= \theta_j-\alpha\left[\left(\frac{1}{m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}
    \right)+\frac{\lambda}{m}\theta_j\right], j\in\{1,2,\dots,n\}
\end{align*}

\noindent We can manipulate these equations to represent our update rule as

\begin{align*}
    \theta_j    &:= \theta_j\left(1-\alpha\frac{\lambda}{m}\right)-\alpha\frac{1}{m}\sum^m_{i=1}
    (h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}
\end{align*}

\noindent The first term, $1-\alpha\frac{\lambda}{m}$ will always be less than 1, hence it reduces the
value of $\theta_j$ by some amount on every update. \\

\noindent \textbf{Regularized Normal Equation:} \\
The equation is the same as before, except with another term inside the parantheses.

\begin{align*}
    \theta          &= \left(X^T X+\lambda\cdot L\right)^{-1} X^T y \\
    \text{where } L &= \begin{bmatrix} 0 & & & & \\ & 1 & & & \\ & & 1 & & \\ & & & \ddots & \\
    & & & & 1 \\ \end{bmatrix}
\end{align*}

\pagebreak
\noindent $L$ has dimensions $(n+1)\times(n+1)$. Intuitively, this is the identity matrix multiplied
by a real number $\lambda$. Recall that if $m<n$ then $X^T X$ is non-invertible, and if $m=n$, then
$X^T X$ may be invertible. However, by adding the term $\lambda\cdot L$, $X^T X+\lamda\cdot L$ becomes
invertible. Hence, if $\lambda>0$, then

\begin{equation*}
    \theta=\left(X^T X +\lambda
    \begin{bmatrix} 0 & & & & \\ & 1 & & & \\ & & 1 & & \\ & & & \ddots & \\
    & & & & 1 \\ \end{bmatrix}\right)^{-1} X^T y
\end{equation*}

\noindent \textbf{Regularized Logistic Cost Function:} \\
The blue term is the only difference from the original logistic cost function.

\begin{equation*}
    J(\theta) = -\frac{1}{m}\sum^m_{i=1}\left[y^{(i)}\log{(h_\theta(x^{(i)}))}
    + (1-y^{(i)})\log{(1-h_\theta(x^{(i)}))}\right]
    \color{blue} + \frac{\lambda}{2m}\sum^n_{j=1} \theta^2_j \color{black}
\end{equation*}