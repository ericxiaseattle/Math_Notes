% Preamble
\documentclass{article}


% Packages
\usepackage[legalpaper, portrati, margin=0.9in]{geometry}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{gensymb}
\usepackage{mathtools}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{pgfplots}
\pgfplotsset{compat=1.17}
\usepgfplotslibrary{fillbetween}
\usepackage{tikz}
\usepackage{tkz-euclide}


% Macros


% File info
\title{My Notes to Andrew Ng's Coursera "Machine Learning"}
\author{Eric Xia}
\date{Last Updated 8 September 2020}


% Document
\begin{document}

    \maketitle
    \tableofcontents

    % Course Schedule
    \begin{center}
        \begin{tabular}{|c|p{60mm}|}
            \hline
            Week 1 & Introduction \newline Univariate Linear Regression \newline Linear Algebra Review \\
            \hline
            Week 2 & Multivariate Linear Regression \newline Computing Parameters Analytically \\
            \hline
            Week 3 & Logistic Regression \newline Regularization \\
            \hline
            Week 4 & Neural Networks: Representation \\
            \hline
            Week 5 & Neural Networks: Learning \\
            \hline
            Week 6 & Advice for Applying Machine Learning \newline Machine Learning System Design \\
            \hline
            Week 7 & Support Vector Machines (SVMs) \\
            \hline
            Week 8 & Unsupervised Learning \newline Dimensionality Reduction \\
            \hline
            Week 9 & Anomaly Detection \newline Recommender Systems \\
            \hline
            Week 10 & Large Scale Machine Learning \\
            \hline
            Week 11 & Application Example: Photo OCR \\
            \hline
        \end{tabular}
    \end{center}

    \pagebreak

    \section{Introduction}
        \noindent \textbf{Machine Learning (ML):} the field of study that gives computers the ability to learn
        without being explicitly programmed \\
        \noindent \textbf{Learning Problems:} a computer program is said to learn from experience E with respect to some
        task T and some performance measure P, if its performance on T, as measured by P, improves with
        experience E. \\

        \noindent \textit{Example: Suppose your email program watches which emails you do or do not mark as spam, and based on
        that learns how to better filter spam. Identify the experience E, task T, and performance measure P
        in this scenario.}
        \noindent \textbf{T:} Classifying emails as spam or not spam. \\
        \noindent \textbf{E:} Watching you label emails as spam or not spam. \\
        \noindent \textbf{P:} The number (or fraction) of emails correctly classified as spam/not spam. \\

        \noindent \textbf{Supervised Learning:} we give the computer a dataset and the right/wrong answers
        and then attempt to produce more correct answers. \\
        \noindent \textbf{Unsupervised Learning:} we approach problems with little to no idea of what our
        results should look like and we derive structure from data where we don't necessarily know the effect
        of the variables; we let the computer learn itself, e.g. organizing computing clusters, social network
        analysis, market segmentation, astronomical data analysis. \\
        
        \noindent \textbf{Clustering:} finding natural groups in the feature space of input data \\
        \noindent \textbf{Regression:} attempting to predict continuous values \\
        \noindent \textbf{Classification:} attemping to predict discrete-valued output or map input variables
        into discrete categories, e.g. Benign vs. Malignant Tumor \\



    \pagebreak
    \section{Univariate Linear Regression}
        \noindent \textbf{Training Set:} the dataset used to train a model \\
        $\bm{m}$: number of training examples \\
        $\bm{x}$'s: input variables/features \\
        $\bm{y}$'s: output variables/features \\
        $\bm{(x,y)}$: one training example \\
        $\bm{(x^{(i)}, y^{(i)})}$: the $i$th training example \\
        \textbf{Hypothesis Function ($h:x\to y,h(x)=h_\theta (x)=\theta_0+\theta_1 x$)}: given a training set,
        we want to choose \textbf{parameters ($\theta_0$ and $\theta_1$)}such that the mean distance between
        $h(x)$ and the training examples are as close to the true mean distance:

        \begin{figure}[hbt!]
            \centering
            \includegraphics[scale=0.2]{Resources/Linear_Regression.png}
            \caption*{\textbf{Linear Regression}}
        \end{figure}

        \begin{figure}[hbt!]
            \centering
            \includegraphics[scale=0.75]{Resources/Hypothesis_Function.png}
            \caption*{\textbf{Hypothesis Function}}
        \end{figure}

        \noindent We can measure the error of a hypothesis function by using a \textbf{cost function} $J$,
        given by

        \begin{align*}
            J(\theta_0,\theta_1)    &= \frac{1}{2m}\sum^m_{i=1}(h_\theta(x_i)-y_i)^2
        \end{align*}

        \noindent This function is really just the average of the differences between the produced outputs
        with the hypothesis function being tested, and the actual outputs. The cost function is also referred
        to as the squared error function. We half the mean in the cost function out of convenience for
        computing the gradient descent, since the derivative term of the square function will cancel out the
        half. \\

        \noindent The contour plot display of the cost function for a particular hypothesis function:

        \begin{figure}[hbt!]
            \centering
            \includegraphics[scale=0.75]{Resources/Contour.PNG}
        \end{figure}

        \noindent \textbf{Gradient Descent:} an optimization algorithm used to minimize some function, e.g. the
        cost function, by iteratively moving in the direction of the steepest descent as defined by the negative
        of the gradient. \\

        \noindent Putting $\theta_0$ on the $x$-axis and $\theta_1$ on the $y$-axis, with the cost function
        on the $z$-axis, the points on the graph are the outputs of our cost function. We know we have succeeded
        in finding parameters for $J(\theta_0,\theta_1)$ that minimize the cost when our gradient descent leads
        us to the blue area of the graph (lowest $z$-value). The red arrows on the graph show the minimum points
        on the graph.

        \begin{figure}[hbt!]
            \centering
            \includegraphics[scale=0.75]{Resources/Gradient_Descent}
        \end{figure}

        \noindent We implement this algorithm by taking the derivative of the cost function. This gives us the
        slope of the cost function at a point, and a direction to move towards. We then iteratively step down
        the cost function in the direction of the steepest descent. The size of each step is determined by the
        \textbf{learning rate}, the parameter $\alpha$. Note that $\alpha$ is fixed and its value need not variate
        because the cost function is parabolic, hence as we approach the minimum, the derivative decreases in
        magnitude and flattens out. \\

        \noindent We want to choose a value of $\alpha$ that is not too small or too large:

        \begin{figure}[hbt!]
            \centering
            \includegraphics[scale=0.5]{Resources/Learning_Rate.PNG}
        \end{figure}

        \noindent Then our implementation of gradient descent is given by repeating the following equation until
        convergence.

        \begin{equation*}
            \theta_j := \theta_j-\alpha\frac{\partial}{\partial \theta_j}J(\theta_0,\theta_1)
        \end{equation*}

        \noindent where $j=0,1$ represents the index and "$:=$" is the assignment operator. \\

        \noindent Note that the gradient descent equations for $\theta_0$ and $\theta_1$ should be updated
        simultaneously, such that updating of $\theta_0$ does not interfere with the accurate updating of $\theta_1$:

        \begin{figure}[hbt!]
            \centering
            \includegraphics[scale=0.75]{Resources/Simultaneous_Update}
        \end{figure}

        \noindent The current gradient descent algorithm we are using is called \textbf{batch gradient descent}
        because it accounts for every training example to determine the descent steps. For a single training
        example,

        \begin{align*}
            \frac{\partial}{\partial\theta_j}J(\theta)  &= \frac{\partial}{\partial\theta_j}\frac{1}{2}(h_\theta(x)-y)^2 \\
                                                        &= 2\cdot\frac{1}{2}(h_\theta(x)-y)\cdot
                                                           \frac{\partial}{\partial\theta_j}(h_\theta(x)-y) \\
                                                        &= (h_\theta(x)-y)\cdot\frac{\partial}{\partial\theta_j}
                                                           \left(\sum^n_{i=0}\theta_i x_i-y\right) \\
                                                        &= (h_\theta(x)-y)x_j
        \end{align*}

        \noindent Hence, we repeat the following equations until convergence.

        \begin{align*}
            \theta_0 &:= \theta_0-\alpha\frac{1}{m}\sum^m_{i=1}(h_\theta(x_i)-y_i) \\
            \theta_1 &:= \theta_1-\alpha\frac{1}{m}\sum^m_{i=1}(h_\theta(x_i)-y)x_i
        \end{align*}

        \noindent While gradient descent can be susceptible to local minima, simple optimization problems for
        linear regression only have one global, and no other local or optima. Thus, gradient descent will always
        converge here to the global minimum. Assuming we have a well-chosen $\alpha$, we see that $J$ is indeed
        a convex quadratic function, and the below contour plot depicts the trajectory of an implementation of
        gradient descent to minimize a quadratic function. The points start from the warm outermost colors and
        move towards the cooler innermost colors, until they converge at the global minimum of the cost function.

        \begin{figure}[hbt!]
            \centering
            \includegraphics[scale=0.75]{Resources/Contour2}
        \end{figure}



    \pagebreak
    \section{Linear Algebra Review}
        \textbf{Matrix:} a 2D array \\
        $\bullet$ The dimension of a matrix is given by "number of rows $\times$ number of columns" \\
        $\bullet$ Dimension may be written $\mathbb{R}^{4\times 2}$, representing a $4\times2$ matrix of the
        real-valued set $\mathbb{R}$ \\
        $\bullet$ In the declaration below, $A_{iy}=$ "$i,j$ entry" in the $i$th row, $j$th column

        \begin{equation*}
            A = \begin{bmatrix}
                    1402 & 191 \\
                    1371 & 821 \\
                    949  & 1437 \\
                    147  & 1448
                \end{bmatrix}
        \end{equation*}

        \noindent \textbf{Vector:} A $n\times1$ matrix \\
        $\bullet$ Can be 1-indexed or 0-indexed, where 1-indexed is more conventional \\
        $\bullet$ It is conventional to name matrices with capital letters and vectors with lowercase letters \\
        $\bullet$ In the declaration below, $y_i=i$th element of the given 4D Vector, $\mathbb{R}^4$

        \begin{equation*}
            y = \begin{bmatrix}
                    460 \\
                    232 \\
                    315 \\
                    178
                \end{bmatrix}
        \end{equation*}

        \noindent \textbf{Matrix Addition:} a defined matrix sum must have two addends with the same dimensions.

        \begin{equation*}
            \begin{bmatrix}
                a_1 & d_1 \\
                b_1 & e_1 \\
                c_1 & f_1
            \end{bmatrix}
            +
            \begin{bmatrix}
                a_2 & d_2 \\
                b_2 & e_2 \\
                c_2 & f_2
            \end{bmatrix}
            =
            \begin{bmatrix}
                a_1 + a_2   &= d_1 + d_2 \\
                b_1 + b_2   &= e_1 + e_2 \\
                c_1 + c_2   &= f_1 + f_2
            \end{bmatrix}
        \end{equation*}

        \noindent \textbf{Scalar Matrix Multiplication:}

        \begin{equation*}
            C   \begin{bmatrix}
                    a & d \\
                    b & e \\
                    c & f
                \end{bmatrix}
            =
                \begin{bmatrix}
                    Ca & Cd \\
                    Cb & Ce \\
                    Cc & Cf
                \end{bmatrix}
        \end{equation*}

        \noindent Division example:

        \begin{equation*}
            \begin{bmatrix}
                4 & 0 \\
                6 & 3
            \end{bmatrix}
            /4
            =
            \frac{1}{4}
            \begin{bmatrix}
                4 & 0 \\
                6 & 3
            \end{bmatrix}
            =
            \begin{bmatrix}
                1   & 0 \\
                3/2 & 3/4
            \end{bmatrix}
        \end{equation*}

        \noindent \textbf{Matrix-Vector Multiplication:} if $A$ is a $m\times n$ matrix then the product $Ax$
        is defined by $n\times 1$ column vectors $x$. If $Ax=b$ then $b$ is a $m\times 1$ column vector.

        \begin{figure}[hbt!]
            \centering
            \includegraphics[scale=0.75]{Resources/Matrix_Vector_Mult}
        \end{figure}

        \noindent Example:

        \begin{equation*}
            \begin{bmatrix}
                1 & -1 & 2 \\
                0 & -3 & 1
            \end{bmatrix}
            \begin{bmatrix}
                2 \\
                1 \\
                0
            \end{bmatrix}
            =
            \begin{bmatrix}
                2\cdot 1    & -1\cdot 1 & 0\cdot 2 \\
                2\cdot 0    & -1\cdot 3 & 0\cdot 1
            \end{bmatrix}
            =
            \begin{bmatrix}
                1 \\
                -3
            \end{bmatrix}
        \end{equation*}

        \noindent \textbf{Matrix-Matrix Multiplication to Combine Hypotheses:} If we wanted to make a
        hypothesis from 3 hypotheses we could represent the hypotheses by matrices and the dataset by another
        matrix and multiply them to get the product matrix which would be the combined hypothesis:

        \begin{figure}[hbt!]
            \centering
            \includegraphics[scale=0.4]{Resources/Hypothesis_Combination}
        \end{figure}

        \pagebreak
        \noindent \textbf{Properties of Matrix-Matrix Multiplication:} \\
        $\bullet$ In general, $A\times B\not = B\times A$ (not commutative) \\
        $\bullet$ $A\times(B\times C)=(A\times B)\times C$ (is associative) \\

        \noindent \textbf{Identity Matrix:} a special square matrix that has 1's along the main diagonal
        (upper left to lower right) and 0's for all other elements \\
        $\bullet$ Denoted $I$ or $I_{n\timesm}$ \\
        $\bullet$ For any matrix $A,A_{m\times n}\cdot I_{n\times n}=I_{m\times m}\cdot A_{m\times n}=
        A_{m\times n}$ (the subscripts are usually omitted) \\
        $\bullet$ $A\timesB=B\timesA$ when $B=I$ (matrices are commutative when at least one of them is the
        identity matrix) \\

        \noindent Example of an identity matrix:

        \begin{equation*}
            I_3 =
            \begin{bmatrix}
                1 & 0 & 0 \\
                0 & 1 & 0 \\
                0 & 0 & 1
            \end{bmatrix}
        \end{equation*}

        \noindent Informally,

        \begin{figure}[hbt!]
            \centering
            \includegraphics[scale=1.2]{Resources/Identity2}
        \end{figure}

        \noindent \textbf{Matrix Inverse:} If $A$ is a $m\times m$ matrix, and if it has an inverse
        (only square matrices have inverses), then $A(A^{-1})=A^{-1}A=I$ \\

        \noindent \textbf{Matrix Transpose:} If $A$ is a $m\times n$ matrix, $B=A^T$ then $B$ is a $n\times m$
        matrix, and $B_{ij}=A_{ij}$ \\
        $\bullet$ Basically, the rows of $A$ become the columns of $B$ and the columns of $A$ become the
        rows of $B$

        \begin{equation*}
            A =
            \begin{bmatrix}
                1 & 2 & 0 \\
                0 & 5 & 6 \\
                7 & 0 & 9
            \end{bmatrix}
            , A^T =
            \begin{bmatrix}
                1 & 0 & 7 \\
                2 & 5 & 0 \\
                0 & 6 & 9
            \end{bmatrix}
        \end{equation*}



    \pagebreak
    \section{Multivariate Linear Regression}
        \textbf{Notation:}
        $\bm{x_j^{(i)}}=$ value of features $j$ in $i$th training example \\
        $\bm{x^{(i)}}=$ input (features) of $i$th training example \\
        $\bm{m}=$ the number of training examples \\
        $\bm{n}=$ the number of features \\

        \noindent We can extend the univarate hypothesis function, $h_\theta(x)=\theta_0+\theta_1 x $ to
        multiple variables like so:

        \begin{equation*}
            h_{\theta}(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_3+\dots+\theta_n x_n
        \end{equation*}

        \noindent We can represent the \textbf{multivariate hypothesis function} by defining $x_0=1$ and
        writing the $x$'s and $\theta$'s as parameter vectors:

        \begin{equation*}
            x = \begin{bmatrix}
                    x_0 \\
                    x_1 \\
                    x_2 \\
                    \vdots \\
                    x_n
                \end{bmatrix}
            \in\mathbb{R}^{n+1}
            ,\theta = \begin{bmatrix}
                          \theta_0 \\
                          \theta_1 \\
                          \theta_2 \\
                          \vdots \\
                          \theta_n
                      \end{bmatrix}
            \in\mathbb{R}^{n+1}
        \end{equation*}

        \noindent Then

        \begin{align*}
            h_{\theta}(x)   &= \theta_0 x_0 + \theta_1 x_1 + \dots + \theta_n x_n \\
                            &= \theta^T x
        \end{align*}

        \noindent where $\theta^T=[\theta_0,\theta_1,\dots,\theta_n]$ and
        $x=\begin{bmatrix}x_0\\x_1\\\vdots\\\x_n\end{bmatrix}$. Together, $\theta^T x$ is read
        "theta transpose $x$". \\

        \noindent \textbf{Multivariate Gradient Descent Equation:}

        \begin{align*}
            \theta_j &:= \theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta) \\
                     &:= \theta_j-\alpha\frac{1}{m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}
        \end{align*}

        \noindent where we simulatenously update $\theta_j$ for $j=0,\dots,n$. Note that the parameter of $J$,
        $\theta$, represents a vector containing all the $\theta$'s from $\theta_0$ to $\theta_n$. \\

        \noindent We can speed up gradient descent by having our input values in roughly the same range. This
        is because $\theta$ descends faster on small ranges and slowly on large ranges so it will oscillate
        inefficiently down to the minimum of the cost function when the variables are uneven. Ideally, we want
        $-1\leq x_{(i)}\leq 1$ or $-0.5\leq x_{(i)}\leq 0.5$. If the range is too large, say, $-3\leq x\leq 3$
        or too small, like $-\frac{1}{3}\leq x\leq\frac{1}{3}$, then the contours of our cost function will
        be really narrow and gradient descent will be more inefficient.

        \begin{figure}[hbt!]
            \centering
            \includegraphics[scale=0.4]{Resources/Contour3}
        \end{figure}

        \noindent One way to prevent gradient descent inefficiency is by \textbf{feature scaling}, which
        involves dividing the input values by the range of the input variable, resulting in a new range of
        simply 1. The following equation demonstrates how we rescale a value of a features.

        \begin{equation*}
            x_{\text{new}} = \frac{x_i-\text{min}(x)}{\text{max}(x)-\text{min}(x)}
        \end{equation*}

        \noindent \textbf{Mean Normalization} is a second way we might rescale features, involving subtracting
        the average value for an input variable from the values for that input variable which result in a new
        average value for the input variable of simply 0. We adjust input values by the following formula.

        \begin{equation*}
            x_i := \frac{x_i - \mu_i}{s_i}
        \end{equation*}

        \noindent where $\mu_i$ is the mean of the feature values and $s_i$ is the range of values or the
        standard deviation. For example, if $x_i$ represents housing prices with a range of 100 to 2000
        and a mean value of 1000, then $x_i := \frac{\text{price}-1000}{1900}$.

        \pagebreak
        \noindent We can debug gradient descent by making a plot with the number of iterations on the $x$-axis.
        Then plotting the cost function over the number of iterations of gradient descent, if $J(\theta)$ ever
        increases, then it is likely that we need to decrease the value of $\alpha$. If $\alpha$ is too large,
        then the cost function may not decrease on every iteration and may diverge. However, if $\alpha$ is
        too small, the convergence may be too slow. \\

        \noindent Common plots that show we need a smaller learning rate include the two figures below.

        \begin{figure}[hbt!]
            \centering
            \begin{subfigure}[b]{.45\linewidth}
                \includegraphics[scale=1]{Resources/Gradient_Descent_Debug}
            \end{subfigure}
            \begin{subfigure}[b]{.45\linewidth}
                \includegraphics[scale=0.8]{Resources/Gradient_Descent_Debug2}
            \end{subfigure}
        \end{figure}

        \noindent We can change the behavior of a hypothesis function by making it a different type of
        function, including quadratic, cubic, and radical if it fits the data better. If our hypothesis
        function is $h_\theta (x)=\theta_0+\theta_1 x_1$ then we can create additional features based on $x_1$
        to get the square root function $h_\theta(x)=\theta_0 +\theta_1 x_1+\theta_2\sqrt{x_1}$ or the cubic
        function $h_\theta(x)=\theta_0 + \theta_1 x_1 + \theta_2 x_1^2 + \theta_3 x_1^3$.



    \pagebreak
    \section{Computing Parameters Analytically}
        \textbf{Normal Equation:} an analytical approach to Linear Regression with a Least Square Cost Function \\
        $\bullet$ Allows us to directly find the parameters $\theta$ without using gradient descent. \\

        \noindent \textbf{A Derivation of the Normal Equation:} \\
        We have our hypothesis function $h(\theta)=\theta^T x$ and cost function $J(\theta)=\frac{1}{2m}
        \sum^m_{i=1}[h_\theta (x^{(i)})-y^{(i)}]^2$. Let us create a matrix $X$ such that

        \begin{align*}
            X =
            \begin{bmatrix}
                x^1 \\
                x^2 \\
                \vdots \\
                x^i
            \end{bmatrix}
        \end{align*}

        \noindent where $X$ is a $m\times 1$ matrix and each row is the $i$th training example. Now we can use
        $X$'s definition to rewrite the cost function $J(\theta)$, ignoring the $\frac{1}{2m}$ because we're
        computing the derivative to zero so it doesn't matter. Then

        \begin{align*}
            J(\theta) &= ((X\theta)^T-y^T)(X\theta-y) \\
            &= (X\theta)^T X\theta - (X\theta)^T y - y^T(X\theta) + y^T y \\
            &= \theta^T X^T X\theta-2(X\theta)^T y+y^T y
        \end{align*}

        \noindent Note that both $X\theta$ and $y$ are vectors, hence, they have the associative property of
        multiplication and we were able to simplify down to the above equation. Since we want to minimize the
        cost function $J(\theta)$, let's set the partial derivative to 0.

        \begin{align*}
            \frac{\partial J(\theta)}{\partial\theta}  = 0
        \end{align*}

        \noindent We can ignore the last term of $J(\theta)$ because its behavior doesn't depend on the variable
        of interest. Let us differentiate the second term of $J(\theta)$, which we will refer to as

        \begin{equation*}
            P(\theta) = 2(X\theta)^T y
        \end{equation*}

        \noindent Recall that $\theta$ is a vector of $n$ components, $y$ is a vector of $m$ components, and
        $X$ is a $m\times n$ matrix.

        \noindent Then the matrix expansion of $P(\theta)$ is

        \begin{align*}
            P(\theta) = 2
            \left[
            \begin{pmatrix}
                x_{11} & x_{12} & \dots & x_{1n} \\
                x_{21} & \dots  & \dots & x_{2n} \\
                \vdots & \vdots & \vdots & \vdots \\
                x_{m1} & \dots & \dots & x_{mn}
            \end{pmatrix}
            \begin{pmatrix}
                \theta_1 \\
                \theta_2 \\
                \vdots \\
                \theta_n
            \end{pmatrix}
            \right]^T
            \begin{pmatrix}
                y_1 \\
                y_2 \\
                \vdots \\
                y_m
            \end{pmatrix}
        \end{align*}

        \noindent Then

        \begin{align*}
            P(x) &= 2
            \left[
            \begin{pmatrix}
                x_{11}\theta_1 + \dots + x_{1n}\theta_n \\
                x_{21}\theta_1 + \dots + x_{2n}\theta_n \\
                \vdots \\
                x_{m1}\theta_1 + \dots + x_{mn}\theta_n
            \end{pmatrix}
            \right]^T
            \begin{pmatrix}
                y_1 \\
                y_2 \\
                \vdots \\
                y_m
            \end{pmatrix} \\
            &= 2(x_{11}\theta_1+\dots+x_{1n}\theta_n)y_1 + 2(x_{21}\theta_1+\dots+x_{2n}\theta_n)y_2
            + \dots + 2(x_{m1}\theta_1+\dots+x_{mn}\theta_n)y_m \\
            &= 2\sum^m_{i=1}y_i(x_{i1}\theta_1 +\dots+ x_{in}\theta_n) \\
            &= 2\sum^m_{i=1}y_i \sum^n_{j=1} x_{ij}\theta_j
        \end{align*}

        \noindent and

        \begin{align*}
            \frac{\partial P}{\partial \theta_1}    &= 2(x_{11}y_1+\dots+x_{m1}y_m) \\
            \frac{\partial P}{\partial \theta_2}    &= 2(x_{12}y_1+\dots+x_{m2}y_m) \\
            \frac{\partial P}{\partial \theta_n}    &= 2(x_{1n}y_1+\dots+x_{mn}y_m)
        \end{align*}

        \noindent Since $\frac{\partial P}{\partial\theta}$ can be thought of as a vector of $n$ components,
        we can rewrite these equations using a matrix-by-vector multiplication, effectively getting

        \begin{equation*}
            \frac{\partial P}{\partial\theta} = 2X^T y
        \end{equation*}

        \noindent We also now know that

        \begin{equation*}
            \frac{\partial}{\partial\theta} (X\theta)^T y = X^T y
        \end{equation*}

        \noindent Going back to $J(\theta) = \theta^T X^T X\theta-2(X\theta)^T y+y^T y$, let us define the
        first term as

        \begin{align*}
            Q(\theta) = \theta^T X^T X\theta
        \end{align*}

        \noindent We expand and transpose $Q(\theta)$ such that

        \begin{align*}
            Q(\theta) = (\theta_1\dots\theta_n)
            \begin{pmatrix}
                x_{11} & x_{21} & \dots & x_{m1} \\
                x_{12} & \dots  & \dots & x_{m2} \\
                \vdots & \vdots & \vdots & \vdots \\
                x_{1n} & \dots  & \dots & x_{mn}
            \end{pmatrix}
            \begin{pmatrix}
                x_{11} & x_{12} & \dots & x_{1n} \\
                x_{21} & \dots  & \dots & x_{2n} \\
                \vdots & \vdots & \vdots & \vdots \\
                x_{m1} & \dots  & \dots & x_{mn}
            \end{pmatrix}
            \begin{pmatrix}
                \theta_1 \\
                \theta_2 \\
                \vdots \\
                \theta_n
            \end{pmatrix}
        \end{align*}

        \noindent Multiplying the two largest matrices together, we have a "$X$-squared matrix" which is
        $n\times n$. The rows $r$ and columns $c$ satisfy the declaration

        \begin{equation*}
            X^2_{rc} = \sum^m_{i=1} x_{ir}x_{ic}
        \end{equation*}

        \noindent We multiply $X^2_{rc}$ by the $\theta$ vector to get

        \begin{align*}
            Q(\theta) &= (\theta_1\dots\theta_n)
            \begin{pmatrix}
                x^2_{11}\theta_1 + \dots + x^2_{1n}\theta_n \\
                x^2_{21}\theta_1 + \dots + x^2_{2n}\theta_n \\
                x^2_{n1}\theta_1 + \dots + x^2_{nn}\theta_n \\
            \end{pmatrix} \\
            &= \theta_1(X^2_{11}\theta_1+\dots+X^2_{1n}\theta_n) + \theta_2(X^2_{21}\theta_1+\dots+X^2_{2n}\theta_n)
            +\dots+\theta_n(X^2{n1}\theta_1+\dots+X^2_{nn}\theta_n)
        \end{align*}

        \noindent We have

        \begin{align*}
            \frac{\partial Q}{\partial \theta_1} = (2\theta_1 x^2_{11}_\theta_2 X^2_{12}+\dots+
            \theta_n X^2_{21}+\dots+\theta_n X^2{n1})
        \end{align*}

        \noindent Since $X^2$ is symmetric, we know that $X^2_{12} = X^2_{21}$, etc. Hence,

        \begin{align*}
            \frac{\partial Q}{\partial \theta_1} &= 2\theta_1 X^2_{11}+2\theta_2 X^2_{12}+\dots+2\theta_n X^2_{1n} \\
            \frac{\partial Q}{\partial \theta} &= 2X^2\theta \\
            \frac{\partial Q}{\partial \theta} &= 2X^T X\theta \\
            \frac{\partial J}{\partial \theta} &= \frac{\partial Q}{\partial\theta}-\frac{\partial P}{\partial\theta} \\
            &= 2X^T X\theta - 2X^T y
        \end{align*}

        \noindent We want to set the partial derivative to 0 in order to find the values of $\theta$ which
        minimize the cost function $J(\theta)$:

        \begin{align*}
            2X^T X\theta - 2X^T y &= 0 \\
            X^T X\theta &= X^T y
        \end{align*}

        \noindent Assume the matrix $X^T X$ is invertible; we can multiply both sides of the above equation
        by $(X^T X)^{-1}$. Then we get the \textbf{normal equation,}

        \begin{equation*}
            \theta = (X^T X)^{-1} X^T y
        \end{equation*}

        \noindent Below is an example of using the normal equation to quickly calculate the desired value of
        $\theta$.

        \begin{figure}[hbt!]
            \centering
            \includegraphics[scale=0.5]{Resources/Norm_Equation_Example}
        \end{figure}

        \pagebreak

        \begin{figure}[hbt!]
            \centering
            \includegraphics[scale=0.4]{Resources/Norm_Equation_Example2}
        \end{figure}

        \begin{figure}[hbt!]
            \centering
            \includegraphics[scale=0.4]{Resources/Norm_Equation_Example3}
        \end{figure}

        \noindent Both gradient descent and using the normal equation have their advantages and disadvantages.
        As a general rule of thumb,

        \begin{center}
            \begin{tabular}{|c|c|}
                \hline
                \textbf{Gradient Descent}       & \textbf{Normal Equation} \\
                \hline
                Need to choose $\alpha$         & No need to choose $\alpha$ \\
                \hline
                Needs many iterations           & No need to iterate \\
                \hline
                $O(kn^2)$                       & $O(n^3)$, need to calculate inverse of $X^T X$ \\
                \hline
                Works well when $n$ is large    & Slow if $n$ is very large \\
                \hline
            \end{tabular}
        \end{center}

        \noindent In practice, it might be a good time to start using an iterative process when $n>10,000$. \\

        \noindent In this class, we want to use the Octave "pinv" function instead of "inv", which will give
        a value of $\theta$ even if $X^T X$ is non-invertible. If $X^T X$ is non-invertible, common causes
        include redundant features, where two features are very closely related (linearly dependent), or
        too many features (e.g. $m\leq n$).



    \pagebreak
    \section{Logistic Regression}


    \section{Regularization}
    \section{Neural Networks: Representation}
    \section{Neural Networks: Learning}
    \section{Advice for Applying Machine Learning}
    \section{Machine Learning System Design}
    \section{Support Vector Machines (SVMs)}
    \section{Unsupervised Learning}
    \section{Dimensionality Reduction}
    \section{Anomaly Detection}
    \section{Recommender Systems}
    \section{Large Scale Machine Learning}
    \section{Application Example: Photo OCR}

\end{document}